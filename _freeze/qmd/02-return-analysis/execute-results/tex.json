{
  "hash": "108b2671a3292f73b4fb1f363b565f6f",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat: html\nexecute:\n  fig-width: 7\n  fig-height: 4\nknitr:\n  opts_chunk:\n    out.width: \"100%\"\n---\n\n# Return Analysis\n\n\n\nThe objective of this chapter is to identify the optimal ARMA(p,q) model to describe the dynamics of ASML log-returns. To determine the most suitable stochastic process, we adopt a systematic approach structured in the following stages:\n\n1. **Exploratory Analysis:** Analysis of autocorrelation functions **(ACF/PACF)** to assess temporal dependency and detect potential seasonality or distinct patterns.\n2. **Model Selection (Grid Search):** Estimation of various  order combinations, residual analysis (plots and **Ljung-Box test**), and selection of the \"winning\" models based on the minimization of **AIC (Akaike Information Criterion)** and **BIC (Bayesian Information Criterion)**.\n3. **Out-of-Sample Validation:** Comparison of the predictive performance of the selected models against a benchmark (historical mean) using a test set withheld from the estimation process.\n\n## ACF and PACF Analysis\n\nAnalyzing correlograms is the preliminary step to identify potential orders for the **AR (AutoRegressive)** and **MA (Moving Average)** components.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacf_res <- acf(log_returns$LogReturns, plot = FALSE, lag.max = 20)\npacf_res <- pacf(log_returns$LogReturns, plot = FALSE, lag.max = 20)\n\nci <- qnorm((1 + 0.95)/2)/sqrt(length(log_returns$LogReturns))\n\ndf_acf <- data.frame(\n  lag = as.numeric(acf_res$lag)[-1],\n  acf = as.numeric(acf_res$acf)[-1])\n\ndf_pacf <- data.frame(\n  lag = as.numeric(pacf_res$lag),\n  pacf = as.numeric(pacf_res$acf))\n\nfig_acf <- plot_ly(df_acf, x = ~lag, y = ~acf, type = 'bar', name = 'ACF',\n  marker = list(color = '#1f77b4')) %>%\n  \n  add_segments(x = min(df_acf$lag), xend = max(df_acf$lag), y = ci, yend = ci, \n  line = list(color = 'red', dash = 'dash', width = 1), showlegend = FALSE) %>%\n  \n  add_segments(x = min(df_acf$lag), xend = max(df_acf$lag), y = -ci, yend = -ci, \n  line = list(color = 'red', dash = 'dash', width = 1), showlegend = FALSE) %>%\n  \n  layout(yaxis = list(title = \"ACF\"))\n\nfig_pacf <- plot_ly(df_pacf, x = ~lag, y = ~pacf, type = 'bar', name = 'PACF',\n  marker = list(color = '#ff7f0e')) %>%\n  \n  add_segments(x = min(df_pacf$lag), xend = max(df_pacf$lag), y = ci, yend = ci, \n  line = list(color = 'red', dash = 'dash', width = 1), showlegend = FALSE) %>%\n  \n  add_segments(x = min(df_pacf$lag), xend = max(df_pacf$lag), y = -ci, yend = -ci, \n  line = list(color = 'red', dash = 'dash', width = 1), showlegend = FALSE) %>%\n  \n  layout(yaxis = list(title = \"PACF\"))\n\nsubplot(fig_acf, fig_pacf, nrows = 1, margin = 0.05, titleY = TRUE) %>%\n  layout(title = \"Autocorrelation & Partial Autocorrelation\", margin = list(t = 50))\n```\n\n::: {.cell-output-display}\n![Log-Returns ACF and PACF](02-return-analysis_files/figure-pdf/acf-pacf-plots-1.pdf){fig-pos='H' width=100%}\n:::\n:::\n\n\n**Observations:** The autocorrelation at lag-1 is statistically significant, yet its magnitude is limited (approximately -0.04). Neither the ACF nor the PACF exhibits a clear decay or a sharp cutoff. This ambiguity makes it difficult to visually identify a pure AR or MA process, suggesting instead a mixed ARMA structure.\n\nA classical iterative approach (\"Box-Jenkins\") would proceed by analyzing the **residuals**:\n\n* If the residuals' ACF shows a sharp cutoff $\\rightarrow$ add an MA term.\n* If the residuals' PACF shows a sharp cutoff $\\rightarrow$ add an AR term.\n* If both plots decay slowly or are unclear $\\rightarrow$ increase both orders.\n\nHowever, given the ambiguity of the observed patterns and considering that high-order lags ($>3$) are uncommon in financial contexts, we opted for a more transparent and systematic approach. We will perform a comprehensive search, testing all possible combinations within a limited range.\n\n## Models\n\nTo assess **the goodness of fit** for each configuration, we analyze two fundamental diagnostic outputs:\n\n* **Residual Analysis (`checkresiduals`)**: This function allows us to verify the absence of autocorrelation in the residuals. In addition to a visual inspection of the plots, the **Ljung-Box Test** is performed.\n  + $H_0$ (Null Hypothesis): The residuals are independently distributed (White Noise).\n  + Interpretation: A p-value $>0.05$ indicates that there is insufficient evidence to reject the null hypothesis. Therefore, the model has adequately captured the temporal structure of the data.\n* **Parameter Significance (`coeftest`)**: This reports the estimated coefficients (*ar* and *ma*) along with their Standard Errors. Using the z-test, we verify whether the coefficients are statistically different from zero (p-value $<0.05$).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n```\n:::\n\n\n::: {.panel-tabset}\n\n### AR(1)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_10 <- Arima(log_returns$LogReturns, order=c(1,0,0))\ncheckresiduals(arma_10)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_10-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(1,0,0) with non-zero mean\nQ* = 39.515, df = 9, p-value = 9.299e-06\n\nModel df: 1.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value Pr(>|z|)   \nar1       -0.03766328  0.01236477 -3.0460 0.002319 **\nintercept  0.00051722  0.00033887  1.5263 0.126937   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### MA(1)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_01 <- Arima(log_returns$LogReturns, order=c(0,0,1))\ncheckresiduals(arma_01)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_01-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(0,0,1) with non-zero mean\nQ* = 39.316, df = 9, p-value = 1.01e-05\n\nModel df: 1.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value Pr(>|z|)   \nma1       -0.03870356  0.01255525 -3.0827 0.002052 **\nintercept  0.00051675  0.00033802  1.5288 0.126326   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### ARMA(1,1)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_11 <- Arima(log_returns$LogReturns, order=c(1,0,1))\ncheckresiduals(arma_11)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_11-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(1,0,1) with non-zero mean\nQ* = 30.019, df = 8, p-value = 0.0002097\n\nModel df: 2.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_11)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value  Pr(>|z|)    \nar1        0.74803078  0.09296953  8.0460 8.556e-16 ***\nma1       -0.78463504  0.08698476 -9.0204 < 2.2e-16 ***\nintercept  0.00049195  0.00030041  1.6376    0.1015    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### AR(2)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_20 <- Arima(log_returns$LogReturns, order=c(2,0,0))\ncheckresiduals(arma_20)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_20-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(2,0,0) with non-zero mean\nQ* = 39.327, df = 8, p-value = 4.275e-06\n\nModel df: 2.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value Pr(>|z|)   \nar1       -0.03811219  0.01237289 -3.0803 0.002068 **\nar2       -0.01178765  0.01237261 -0.9527 0.340731   \nintercept  0.00051767  0.00033491  1.5457 0.122176   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### MA(2)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_02 <- Arima(log_returns$LogReturns, order=c(0,0,2))\ncheckresiduals(arma_02)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_02-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(0,0,2) with non-zero mean\nQ* = 38.929, df = 8, p-value = 5.066e-06\n\nModel df: 2.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_02)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value Pr(>|z|)   \nma1       -0.03931359  0.01239570 -3.1716 0.001516 **\nma2       -0.01363511  0.01243057 -1.0969 0.272685   \nintercept  0.00051676  0.00033299  1.5519 0.120694   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### ARMA(1,2)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_12 <- Arima(log_returns$LogReturns, order=c(1,0,2))\ncheckresiduals(arma_12)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_12-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(1,0,2) with non-zero mean\nQ* = 29.667, df = 7, p-value = 0.0001093\n\nModel df: 3.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_12)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value  Pr(>|z|)    \nar1        0.74660442  0.12291149  6.0743 1.245e-09 ***\nma1       -0.78624421  0.12342306 -6.3703 1.886e-10 ***\nma2        0.00387487  0.01562497  0.2480    0.8041    \nintercept  0.00052359  0.00030184  1.7346    0.0828 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### ARMA(2,1)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_21 <- Arima(log_returns$LogReturns, order=c(2,0,1))\ncheckresiduals(arma_21)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_21-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(2,0,1) with non-zero mean\nQ* = 29.433, df = 7, p-value = 0.0001206\n\nModel df: 3.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value  Pr(>|z|)    \nar1        0.75216134  0.10514098  7.1538 8.439e-13 ***\nar2        0.00505353  0.01520891  0.3323   0.73968    \nma1       -0.79218013  0.10452116 -7.5791 3.479e-14 ***\nintercept  0.00052281  0.00030083  1.7379   0.08223 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### ARMA(2,2)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_22 <- Arima(log_returns$LogReturns, order=c(2,0,2))\ncheckresiduals(arma_22)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_22-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(2,0,2) with non-zero mean\nQ* = 27.067, df = 6, p-value = 0.0001407\n\nModel df: 4.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_22)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value  Pr(>|z|)    \nar1        0.13889774  0.20808368  0.6675 0.5044470    \nar2        0.49269101  0.13447436  3.6638 0.0002485 ***\nma1       -0.18349571  0.20966818 -0.8752 0.3814804    \nma2       -0.50222839  0.14273213 -3.5187 0.0004337 ***\nintercept  0.00052958  0.00029978  1.7666 0.0772978 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n### AR(3)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_30 <- Arima(log_returns$LogReturns, order=c(3,0,0))\ncheckresiduals(arma_30)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_30-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(3,0,0) with non-zero mean\nQ* = 27.506, df = 7, p-value = 0.0002702\n\nModel df: 3.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_30)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value  Pr(>|z|)    \nar1       -0.03863782  0.01236319 -3.1252 0.0017767 ** \nar2       -0.01340434  0.01237126 -1.0835 0.2785840    \nar3       -0.04157144  0.01236735 -3.3614 0.0007755 ***\nintercept  0.00051836  0.00032127  1.6135 0.1066419    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### MA(3)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_03 <- Arima(log_returns$LogReturns, order=c(0,0,3))\ncheckresiduals(arma_03)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_03-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(0,0,3) with non-zero mean\nQ* = 26.596, df = 7, p-value = 0.000394\n\nModel df: 3.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_03)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value  Pr(>|z|)    \nma1       -0.03884123  0.01237078 -3.1398 0.0016909 ** \nma2       -0.01337127  0.01235618 -1.0822 0.2791849    \nma3       -0.04358374  0.01269756 -3.4325 0.0005982 ***\nintercept  0.00051686  0.00031767  1.6270 0.1037265    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### ARMA(1,3)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_13 <- Arima(log_returns$LogReturns, order=c(1,0,3))\ncheckresiduals(arma_13)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_13-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(1,0,3) with non-zero mean\nQ* = 27.07, df = 6, p-value = 0.0001405\n\nModel df: 4.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_13)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in sqrt(diag(se)): NaNs produced\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value Pr(>|z|)  \nar1        0.33895106         NaN     NaN      NaN  \nma1       -0.37787376         NaN     NaN      NaN  \nma2       -0.00033557         NaN     NaN      NaN  \nma3       -0.03535024         NaN     NaN      NaN  \nintercept  0.00051530  0.00031168  1.6533  0.09828 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### ARMA(2,3)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_23 <- Arima(log_returns$LogReturns, order=c(2,0,3))\ncheckresiduals(arma_23)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_23-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(2,0,3) with non-zero mean\nQ* = 25.221, df = 5, p-value = 0.0001263\n\nModel df: 5.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_23)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value Pr(>|z|)  \nar1        0.12928859  0.26476852  0.4883  0.62533  \nar2        0.22589080  0.19486455  1.1592  0.24637  \nma1       -0.16826815  0.26487623 -0.6353  0.52525  \nma2       -0.23309281  0.19068257 -1.2224  0.22155  \nma3       -0.03297002  0.01859016 -1.7735  0.07614 .\nintercept  0.00050770  0.00030819  1.6474  0.09948 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### ARMA(3,1)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_31 <- Arima(log_returns$LogReturns, order=c(3,0,1))\ncheckresiduals(arma_31)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_31-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(3,0,1) with non-zero mean\nQ* = 27.5, df = 6, p-value = 0.0001167\n\nModel df: 4.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_31)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error z value Pr(>|z|)   \nar1       -0.01946408  0.55210686 -0.0353 0.971877   \nar2       -0.01274576  0.02435614 -0.5233 0.600760   \nar3       -0.04139634  0.01342376 -3.0838 0.002044 **\nma1       -0.01920247  0.55288877 -0.0347 0.972294   \nintercept  0.00051900  0.00032097  1.6170 0.105887   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### ARMA(3,2)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_32 <- Arima(log_returns$LogReturns, order=c(3,0,2))\ncheckresiduals(arma_32)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_32-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(3,0,2) with non-zero mean\nQ* = 40.334, df = 5, p-value = 1.279e-07\n\nModel df: 5.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_32)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error  z value  Pr(>|z|)    \nar1        0.03147753  0.02132298   1.4762  0.139883    \nar2       -0.97055711  0.01697418 -57.1784 < 2.2e-16 ***\nar3       -0.03525799  0.01280948  -2.7525  0.005914 ** \nma1       -0.06976751  0.01731179  -4.0301 5.576e-05 ***\nma2        0.96109526  0.02058307  46.6935 < 2.2e-16 ***\nintercept  0.00053285  0.00033637   1.5841  0.113169    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### ARMA(3,3)\n\n::: {.cell}\n\n```{.r .cell-code}\narma_33 <- Arima(log_returns$LogReturns, order=c(3,0,3))\ncheckresiduals(arma_33)\n```\n\n::: {.cell-output-display}\n![](02-return-analysis_files/figure-pdf/arma_33-1.pdf){fig-pos='H' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(3,0,3) with non-zero mean\nQ* = 8.5727, df = 4, p-value = 0.07271\n\nModel df: 6.   Total lags used: 10\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(arma_33)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in sqrt(diag(se)): NaNs produced\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n             Estimate  Std. Error  z value Pr(>|z|)    \nar1       -0.93303265         NaN      NaN      NaN    \nar2        0.68686580  0.06141265  11.1844  < 2e-16 ***\nar3        0.81111732  0.07456554  10.8779  < 2e-16 ***\nma1        0.89063594         NaN      NaN      NaN    \nma2       -0.72881204  0.05908310 -12.3354  < 2e-16 ***\nma3       -0.80508569  0.07150341 -11.2594  < 2e-16 ***\nintercept  0.00060076  0.00028783   2.0872  0.03687 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n:::\n\nAs observed from the outputs, the simpler models (such as AR(1) and MA(1)) yield very low p-values for the Ljung-Box test ($<0.05$). This indicates that significant autocorrelation remains within the residuals, implying that these models have failed to capture all the relevant information.\n\nConversely, as we increase complexity towards an **ARMA(3,3)**, the p-value rises above the critical 5% threshold. Consequently, we fail to reject the null hypothesis: the residuals of this model behave as **White Noise**. This suggests that the ARMA(3,3) is mathematically effective in capturing the patterns of the time series, although its high complexity warrants caution regarding the risk of **overfitting**.\n\nAdditionally, the **ARMA(1,3)** model exhibits instability in parameter estimation (incalculable Std. Error), pointing to potential **over-parameterization**.\n\n## AIC vs BIC\n\nWe visualize the trends of the information criteria for all estimated models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels_list <- list(\n  \"AR(1)\" = arma_10, \"MA(1)\" = arma_01, \"ARMA(1,1)\" = arma_11,\n  \"AR(2)\" = arma_20, \"MA(2)\" = arma_02, \"ARMA(1,2)\" = arma_12,\n  \"ARMA(2,1)\" = arma_21, \"ARMA(2,2)\" = arma_22,\n  \"AR(3)\" = arma_30, \"MA(3)\" = arma_03, \"ARMA(1,3)\" = arma_13,\n  \"ARMA(2,3)\" = arma_23, \"ARMA(3,1)\" = arma_31, \"ARMA(3,2)\" = arma_32,\n  \"ARMA(3,3)\" = arma_33\n)\n\nmetrics_df <- data.frame(\n  Model = names(models_list),\n  AIC = sapply(models_list, AIC),\n  BIC = sapply(models_list, BIC)\n)\n\nmetrics_df$Model <- factor(metrics_df$Model, levels = metrics_df$Model)\n\nmin_aic_val <- min(metrics_df$AIC)\nmin_bic_val <- min(metrics_df$BIC)\n\nplot_aic <- plot_ly(metrics_df, x = ~Model, y = ~AIC, name = 'AIC',\n  type = 'scatter', mode = 'lines+markers',\n  line = list(color = '#1f77b4', width = 2),\n  marker = list(size = 8, color = '#1f77b4')) %>% \n  layout(title = \"AIC Trend\", yaxis = list(title = \"AIC\"))\n\nbest_aic <- metrics_df[metrics_df$AIC == min_aic_val, ]\nplot_aic <- plot_aic %>%\n  add_trace(data = best_aic, x = ~Model, y = ~AIC, \n    type = 'scatter', mode = 'markers', name = 'Best AIC',\n    marker = list(color = 'red', size = 12, symbol = 'star'),\n    showlegend = FALSE\n  )\n\nplot_bic <- plot_ly(metrics_df, x = ~Model, y = ~BIC, name = 'BIC',\n  type = 'scatter', mode = 'lines+markers',\n  line = list(color = '#ff7f0e', width = 2),\n  marker = list(size = 8, color = '#ff7f0e')) %>% \n  layout(title = \"BIC Trend\", yaxis = list(title = \"BIC\"))\n\nbest_bic <- metrics_df[metrics_df$BIC == min_bic_val, ]\nplot_bic <- plot_bic %>%\n  add_trace(\n    data = best_bic, x = ~Model, y = ~BIC, \n    type = 'scatter', mode = 'markers', name = 'Best BIC',\n    marker = list(color = 'red', size = 12, symbol = 'star'),\n    showlegend = FALSE\n  )\n\nfinal_plot <- subplot(plot_aic, plot_bic, nrows = 2, shareX = TRUE, titleY = TRUE) %>%\n  layout(\n    title = \"Model Selection Criteria: AIC & BIC Trends\",\n    hovermode = \"x unified\",\n    margin = list(t = 50)\n  )\n\nfinal_plot\n```\n\n::: {.cell-output-display}\n![Comparison between AIC and BIC](02-return-analysis_files/figure-pdf/model-comparison-1.pdf){fig-pos='H' width=100%}\n:::\n:::\n\n\nThe graphical analysis of the information criteria highlights a typical divergence found in financial time series modeling.\n\nAs evidenced by the plot, the **AIC** criterion, which prioritizes **goodness of fit**, identifies the **ARMA(3,3)** as the optimal model (minimum value). Conversely, the **BIC** criterion, which penalizes model complexity more severely to prevent overfitting, suggests a much more **parsimonious** model: the **ARMA(1,1)**.\n\nFaced with these conflicting indications (a complex model vs a simple model), it is not possible to determine a priori which one is superior. To resolve this ambiguity and assess which model generalizes better on unobserved data, we will proceed with an **Out-of-Sample validation**, computing forecasts and comparing error metrics against actual data.\n\n## Forecast\n\nSince the information criteria (AIC and BIC) suggest different models, we proceed with an Out-of-Sample validation to test the actual predictive capacity on data not used for estimation.\n\nWe split the time series into two subsets:\n\n* **Training Set ($80\\%$)**: Used to estimate the model parameters.\n* **Test Set ($20\\%$)**: Used to compare forecasts against actual data (h-step-ahead forecast).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_total <- nrow(log_returns)\nn_train <- floor(0.80 * n_total)\n\ntrain_data <- log_returns$LogReturns[1:n_train]\ntest_data <- log_returns$LogReturns[(n_train + 1):n_total]\n\nset.seed(123)\n\nmod_winner_bic <- Arima(train_data, order=c(1,0,1))\nmod_winner_aic <- Arima(train_data, order=c(3,0,3))\nmod_naive <- Arima(train_data, order=c(0,0,0)) \n\nh_steps <- length(test_data)\n\nfc_bic <- forecast(mod_winner_bic, h=h_steps)\nfc_aic <- forecast(mod_winner_aic, h=h_steps)\nfc_naive <- forecast(mod_naive, h=h_steps)\n```\n:::\n\n\nThe following plot displays the time series trend and the forecasts generated by the three models over the test period.\n\n::: {.callout-note}\nGiven the stationary nature and high volatility of log-returns, it is expected that medium-to-long-term forecasts will tend to converge rapidly towards the unconditional mean of the process.\nFor this reason we apply method as **one-step-ahead** and **k-step-ahead**.\n:::\n\n::: {.panel-tabset}\n\n### One-Steap-Ahead\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_bic_rolling <- Arima(log_returns$LogReturns, model = mod_winner_bic)\none_step_bic <- fitted(fit_bic_rolling)[(n_train + 1):n_total]\n\nfit_aic_rolling <- Arima(log_returns$LogReturns, model = mod_winner_aic)\none_step_aic <- fitted(fit_aic_rolling)[(n_train + 1):n_total]\n\ndates_test  <- log_returns$Date[(n_train + 1):n_total]\n\nfig_roll <- plot_ly(\n  x = dates_test, \n  y = test_data, \n  type = 'scatter', \n  mode = 'lines', \n  name = 'Actual Test Data', \n  line = list(color = 'black', width = 1, dash = 'dot')\n)\n\nfig_roll <- add_trace(\n  fig_roll, \n  x = dates_test, \n  y = as.numeric(one_step_bic), \n  name = 'Rolling ARMA(1,1)', \n  line = list(color = 'blue', width = 1)\n)\n\nfig_roll <- add_trace(\n  fig_roll, \n  x = dates_test, \n  y = as.numeric(one_step_aic), \n  name = 'Rolling ARMA(3,3)', \n  line = list(color = 'red', width = 1)\n)\n\nfig_roll <- layout(\n  fig_roll,\n  title = \"One-Step-Ahead Rolling Forecast (Test Set: 20%)\",\n  xaxis = list(title = \"Date\"),\n  yaxis = list(title = \"Log Return\"),\n  legend = list(orientation = \"h\", x = 0.1, y = -0.2)\n)\n\nfig_roll\n```\n\n::: {.cell-output-display}\n![Comparison One-Step-Ahead Rolling Forecast](02-return-analysis_files/figure-pdf/rolling-forecast-1.pdf){fig-pos='H' width=100%}\n:::\n:::\n\n\n### Five-Steap-Ahead\n\n::: {.cell}\n\n```{.r .cell-code}\ncalc_k_step_rolling <- function(model_obj, series, n_train, k=5) {\n  n_total <- length(series)\n  test_indices <- (n_train + 1):n_total\n  forecasts <- numeric(length(test_indices))\n  \n  for (i in seq_along(test_indices)) {\n    target_idx <- test_indices[i]\n    origin_idx <- target_idx - k\n    \n    if (origin_idx > 0) {\n      subset_data <- series[1:origin_idx]\n      fit_temp <- Arima(subset_data, model = model_obj)\n      \n      fc_temp <- forecast(fit_temp, h = k)\n      \n      forecasts[i] <- as.numeric(fc_temp$mean[k])\n    } else {\n      forecasts[i] <- NA\n    }\n  }\n  return(forecasts)\n}\n\nk_horizon <- 5\nvec_5step_bic <- calc_k_step_rolling(mod_winner_bic, log_returns$LogReturns, n_train, k = k_horizon)\nvec_5step_aic <- calc_k_step_rolling(mod_winner_aic, log_returns$LogReturns, n_train, k = k_horizon)\n\ndates_test <- log_returns$Date[(n_train + 1):n_total]\n\nfig_roll_5 <- plot_ly(\n  x = dates_test, \n  y = test_data, \n  type = 'scatter', \n  mode = 'lines', \n  name = 'Actual Test Data', \n  line = list(color = 'black', width = 1, dash = 'dot')\n)\n\nfig_roll_5 <- add_trace(\n  fig_roll_5, \n  x = dates_test, \n  y = vec_5step_bic, \n  name = \"Rolling ARMA(1,1)\", \n  line = list(color = 'blue', width = 1.5)\n)\n\nfig_roll_5 <- add_trace(\n  fig_roll_5, \n  x = dates_test, \n  y = vec_5step_aic, \n  name = \"Rolling ARMA(3,3)\", \n  line = list(color = 'red', width = 1.5)\n)\n\nfig_roll_5 <- layout(\n  fig_roll_5,\n  title = \"Five-Step-Ahead Rolling Forecast (Test Set: 20%)\",\n  xaxis = list(title = \"Date\"),\n  yaxis = list(title = \"Log Return\"),\n  legend = list(orientation = \"h\", x = 0.1, y = -0.2)\n)\n\nfig_roll_5\n```\n\n::: {.cell-output-display}\n![Comparison Five-Step-Ahead Rolling Forecast](02-return-analysis_files/figure-pdf/rolling-forecast-5step-1.pdf){fig-pos='H' width=100%}\n:::\n:::\n\n\n:::\nWe evaluate the predictive performance using:\n\n* **MAE (Mean absolute Error)**\n* **RMSE (Root Mean Squared Error)**\n* **Theil's U2 index:** The ratio between the model's RMSE and the Naive model's RMSE.\n  + $U<1$: The model outperforms the benchmark.\n  + $U \\approx 1$: The model performs equivalently to the benchmark.\n  + $U>1$: The model performs worse than the benchmark.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_metrics_robust <- function(forecast_obj, actuals) {\n  vec_pred <- as.numeric(forecast_obj$mean)\n  vec_obs <- as.numeric(actuals)\n  \n  acc <- accuracy(vec_pred, vec_obs)\n  \n  return(c(RMSE = acc[1, \"RMSE\"], MAE = acc[1, \"MAE\"]))\n}\n\nres_bic <- get_metrics_robust(fc_bic, test_data)\nres_aic <- get_metrics_robust(fc_aic, test_data)\nres_naive <- get_metrics_robust(fc_naive, test_data)\n\nu_bic <- res_bic[\"RMSE\"] / res_naive[\"RMSE\"]\nu_aic <- res_aic[\"RMSE\"] / res_naive[\"RMSE\"]\nu_naive <- 1.0\n\nvalidation_table <- data.frame(\n  Model = c(\"ARMA(1,1) [BIC Winner]\", \"ARMA(3,3) [AIC Winner]\", \"Naive (Benchmark)\"),\n  RMSE = c(res_bic[\"RMSE\"], res_aic[\"RMSE\"], res_naive[\"RMSE\"]),\n  MAE = c(res_bic[\"MAE\"], res_aic[\"MAE\"], res_naive[\"MAE\"]),\n  Theil_U2 = c(u_bic, u_aic, u_naive)\n)\n\ndatatable(validation_table, options = list(dom = 't'), rownames = FALSE)\n```\n\n::: {.cell-output-display}\n![Comparison of out-of-sample error metrics](02-return-analysis_files/figure-pdf/forecast-validation-1.pdf){fig-pos='H' width=100%}\n:::\n:::\n\n\nAs highlighted in the table, the **Theil's U2 index** for both ARMA models is extremely close to 1. This indicates that the statistical models, despite their complexity, fail to significantly outperform the simple forecast based on the historical mean (Naive Benchmark).\n\n::: {.callout-important}\nThis result is consistent with the **Efficient Market Hypothesis (EMH)**, according to which past returns contain little to no useful information for predicting future short-term returns, making the series resemble a Random Walk.\n:::\n\n## Conclusion\n\nThe analysis conducted in this chapter has highlighted the limitations of stationary linear models in forecasting the direction of ASML returns.\n\nAlthough information criteria (AIC) identified the ARMA(3,3) as a mathematical structure capable of \"whitening\" the in-sample residuals, the *Out-of-Sample* validation delivered an unequivocal verdict: **a Theil's U index close to 1 confirms that no ARMA model systematically outperforms the Naive benchmark.** This empirical result corroborates the hypothesis that, regarding the **conditional mean**, returns follow a process closely resembling a Random Walk, rendering trading strategies based solely on past autocorrelation futile.\n\nHowever, the unpredictability of the mean does not imply a total absence of structure within the series. Visual inspection of the log-returns plot reveals evident **volatility clustering**: periods of high volatility tend to be followed by high volatility, and periods of calm by calm (a phenomenon known as *conditional heteroskedasticity*).\n\nBy assuming constant variance over time (homoskedasticity), ARMA models fail to capture this fundamental characteristic of financial markets. Therefore, in the next chapter, we will shift our focus from predicting the *sign* of returns to modeling their **magnitude** (risk) by introducing the **GARCH** (Generalized AutoRegressive Conditional Heteroskedasticity) family of models.",
    "supporting": [
      "02-return-analysis_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}